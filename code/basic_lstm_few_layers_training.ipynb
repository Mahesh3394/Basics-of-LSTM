{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import sys\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train data\n",
    "\n",
    "with open('../data/output_train.txt','r',encoding='utf-8') as f:\n",
    "    train_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select subset of train data\n",
    "\n",
    "raw_text = train_data[:150000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be used for encoding and decoding\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# dump chars list into pickle\n",
    "with open('../data/chars.pkl', 'wb') as f:\n",
    "    pickle.dump(chars, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1711\n",
      "Total Vocab:  61\n"
     ]
    }
   ],
   "source": [
    "# find stats for encoding\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a basic preprocessing steps\n",
    "# https://stackoverflow.com/questions/43018030/replace-apostrophe-short-words-in-python\n",
    "# https://stackoverflow.com/questions/49073673/include-punctuation-in-keras-tokenizer\n",
    "\n",
    "def nlp_preprocessing(total_text):\n",
    "    if type(total_text) is not int:\n",
    "    \n",
    "        # replace multiple spaces with single space\n",
    "        total_text = re.sub('\\s+', ' ', total_text)\n",
    "#         total_text = re.sub('\\s+',' ', total_text)\n",
    "        \n",
    "        #This is to be done because I want to Include Period and Question mark in keras tokenizer.\n",
    "        #I do not want the Tokenizer API to remove them\n",
    "        total_text = total_text.replace(\".\", \" .\")\n",
    "        total_text = total_text.replace(\"?\", \" ?\")\n",
    "        \n",
    "        # specific\n",
    "        total_text = re.sub(r\"won\\'t\", \"will not\", total_text)\n",
    "        total_text = re.sub(r\"can\\'t\", \"can not\", total_text)\n",
    "        total_text = re.sub(r\"\\x00\", \"\", total_text)\n",
    "        # general\n",
    "        total_text = re.sub(r\"n\\'t\", \" not\", total_text)\n",
    "        total_text = re.sub(r\"\\'re\", \" are\", total_text)\n",
    "        total_text = re.sub(r\"\\'s\", \" is\", total_text)\n",
    "        total_text = re.sub(r\"\\'d\", \" would\", total_text)\n",
    "        total_text = re.sub(r\"\\'ll\", \" will\", total_text)\n",
    "        total_text = re.sub(r\"\\'t\", \" not\", total_text)\n",
    "        total_text = re.sub(r\"\\'ve\", \" have\", total_text)\n",
    "        total_text = re.sub(r\"\\'m\", \" am\", total_text)\n",
    "        # converting all the chars into lower-case.\n",
    "        total_text = total_text.lower().strip()\n",
    "        \n",
    "        \n",
    "        return total_text\n",
    "    \n",
    "raw_text = nlp_preprocessing(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  1611\n"
     ]
    }
   ],
   "source": [
    "\n",
    "...\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the LSTM model\n",
    "# define model with one layer and 256 nodes of lstm\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 256)               264192    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 61)                15677     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 279,869\n",
      "Trainable params: 279,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../model/model_train.h5'):\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    history = model.fit(X,y, batch_size=128, epochs=20 , validation_split=0.20, callbacks=[es])\n",
    "    model.save('../model/model_train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the characters\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "# to check inference of the model\n",
    "\n",
    "while True:\n",
    "    seq = input(\"Enter a word : \")\n",
    "    seq = nlp_preprocessing(seq)\n",
    "    pattern = [char_to_int[char] for char in seq]\n",
    "#     seed_ = np.pad(pattern, (0, seq_length - len(pattern)))\n",
    "#     pattern = np.reshape(pattern, (1, seq_length, 1))\n",
    "    print(\"Seed:\")\n",
    "    print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "    # generate characters\n",
    "    for i in range(100):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(n_vocab)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_char[index]\n",
    "        seq_in = [int_to_char[value] for value in pattern]\n",
    "        sys.stdout.write(result)\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.2.0",
   "language": "python",
   "name": "tf2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
