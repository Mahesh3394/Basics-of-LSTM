{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 982.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# create repo\n",
    "\n",
    "folders = os.listdir('/home/cgiuser/MK/learning/LSTM/data/maildir')\n",
    "final_mails = []\n",
    "for f_name in tqdm(folders):\n",
    "    text_dir = f'/home/cgiuser/MK/learning/LSTM/data/maildir/{f_name}/inbox/*.*'\n",
    "    file_names = glob.glob(text_dir)\n",
    "    final_mails.extend(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41509/41509 [00:01<00:00, 27829.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI'll call him but the answer is ..the same...</td>\n",
       "      <td>\\t\\n\\nHey, got a call from Jeff Smiron. Appare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nChip - please put the following transactio...</td>\n",
       "      <td>Centana Storage Arrangement\\n\\n\\n\\nChris/Joe\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nThe only one I have is for Saturday. It is...</td>\n",
       "      <td>\\tRE: Feb Gas Sales\\n\\ncool.  have you gotten ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tRE: Feb Gas Sales\\n\\ncool.  have you gotten ...</td>\n",
       "      <td>\\tRE: Feb Gas Sales\\n\\nHe has already called m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\tRE: Feb Gas Sales\\n\\nHe has already called m...</td>\n",
       "      <td>\\tFeb Gas Sales\\n\\nKen Fowler is the man at TX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  \\n\\nI'll call him but the answer is ..the same...   \n",
       "1  \\n\\nChip - please put the following transactio...   \n",
       "2  \\n\\nThe only one I have is for Saturday. It is...   \n",
       "3  \\tRE: Feb Gas Sales\\n\\ncool.  have you gotten ...   \n",
       "4  \\tRE: Feb Gas Sales\\n\\nHe has already called m...   \n",
       "\n",
       "                                          Sentence_2  \n",
       "0  \\t\\n\\nHey, got a call from Jeff Smiron. Appare...  \n",
       "1   Centana Storage Arrangement\\n\\n\\n\\nChris/Joe\\...  \n",
       "2  \\tRE: Feb Gas Sales\\n\\ncool.  have you gotten ...  \n",
       "3  \\tRE: Feb Gas Sales\\n\\nHe has already called m...  \n",
       "4  \\tFeb Gas Sales\\n\\nKen Fowler is the man at TX...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = []\n",
    "for mail in tqdm(final_mails):\n",
    "    try:\n",
    "        with open(mail,'r',encoding='utf-8') as f:\n",
    "            check = f.read()\n",
    "        msgs = check.split(' -----Original Message-----\\n')\n",
    "\n",
    "        for i in range(len(msgs)-1):\n",
    "            if i == 0:\n",
    "                final.append([msgs[i].split('.pst')[-1],msgs[i+1].split('Subject:')[-1]])\n",
    "            else:\n",
    "                final.append([msgs[i].split('Subject:')[-1],msgs[i+1].split('Subject:')[-1]])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "email_data = pd.DataFrame(final,columns=['Sentence_1','Sentence_2'])\n",
    "\n",
    "email_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i will call him but the answer is  .</td>\n",
       "      <td>hey, got a call from jeff smiron .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chip - please put the following transaction in...</td>\n",
       "      <td>centana storage arrangement chris/joe attached...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0               i will call him but the answer is  .   \n",
       "1  chip - please put the following transaction in...   \n",
       "\n",
       "                                          Sentence_2  \n",
       "0                 hey, got a call from jeff smiron .  \n",
       "1  centana storage arrangement chris/joe attached...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# https://stackoverflow.com/questions/43018030/replace-apostrophe-short-words-in-python\n",
    "# https://stackoverflow.com/questions/49073673/include-punctuation-in-keras-tokenizer\n",
    "\n",
    "def nlp_preprocessing(total_text):\n",
    "    if type(total_text) is not int:\n",
    "        \n",
    "        result1 = total_text.find('.')\n",
    "        result2 = total_text.find('?')\n",
    "        if result1 != -1 and result2 != -1:\n",
    "            limit = min(result1,result2)\n",
    "        elif result1 != -1 and result2 == -1:\n",
    "            limit = result1\n",
    "        else:\n",
    "            limit = result2\n",
    "\n",
    "        total_text =  total_text[0:limit+1]\n",
    "        # replace multiple spaces with single space\n",
    "        total_text = re.sub('\\s+', ' ', total_text)\n",
    "#         total_text = re.sub('\\s+',' ', total_text)\n",
    "        \n",
    "        #This is to be done because I want to Include Period and Question mark in keras tokenizer.\n",
    "        #I do not want the Tokenizer API to remove them\n",
    "        total_text = total_text.replace(\".\", \" .\")\n",
    "        total_text = total_text.replace(\"?\", \" ?\")\n",
    "        \n",
    "        # specific\n",
    "        total_text = re.sub(r\"won\\'t\", \"will not\", total_text)\n",
    "        total_text = re.sub(r\"can\\'t\", \"can not\", total_text)\n",
    "\n",
    "        # general\n",
    "        total_text = re.sub(r\"n\\'t\", \" not\", total_text)\n",
    "        total_text = re.sub(r\"\\'re\", \" are\", total_text)\n",
    "        total_text = re.sub(r\"\\'s\", \" is\", total_text)\n",
    "        total_text = re.sub(r\"\\'d\", \" would\", total_text)\n",
    "        total_text = re.sub(r\"\\'ll\", \" will\", total_text)\n",
    "        total_text = re.sub(r\"\\'t\", \" not\", total_text)\n",
    "        total_text = re.sub(r\"\\'ve\", \" have\", total_text)\n",
    "        total_text = re.sub(r\"\\'m\", \" am\", total_text)\n",
    "        # converting all the chars into lower-case.\n",
    "        total_text = total_text.lower().strip()\n",
    "        \n",
    "        \n",
    "        return total_text\n",
    "    \n",
    "email_data['Sentence_1'] = email_data['Sentence_1'].apply(nlp_preprocessing)\n",
    "email_data['Sentence_2'] = email_data['Sentence_2'].apply(nlp_preprocessing)\n",
    "\n",
    "email_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey, got a call from jeff smiron .'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_data.Sentence_2.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13674, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13662, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the rows having Question or Answer length (character length) > 1000 characters\n",
    "list_index = []\n",
    "for ind in email_data.index:\n",
    "    if len(email_data['Sentence_1'][ind]) > 1000 or len(email_data['Sentence_2'][ind]) > 1000:\n",
    "        list_index.append(ind)\n",
    "    \n",
    "# Dropping the above calculated Indexes\n",
    "email_data.drop(list_index, inplace=True)\n",
    "email_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = email_data.Sentence_1.to_list()\n",
    "questions = email_data.Sentence_2.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#PRARAMBH means the begenning of a Sentence \n",
    "#SAMAAPT means the end of a sentence\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "answers_with_tags = list()\n",
    "for i in range(len(answers)) :\n",
    "\n",
    "    answers_with_tags.append('PRARAMBH '+ answers[i] +' SAMAAPT')\n",
    "\n",
    "\n",
    "# I want the Tokenizer to remove all speacial characters except \".\" and \"?\"\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>@[\\\\]^_`{|}~\\t\\n') #All without \".\" and \"?\"\n",
    "tokenizer.fit_on_texts(questions + answers_with_tags )\n",
    "\n",
    "# word_index is a dixtionary containing word and their index\n",
    "# adding 1 to VOCAB_SIZE because word_index starts from 0\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30094"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13662, 55) 55\n"
     ]
    }
   ],
   "source": [
    "# Creating data from Encodder Input - encoder_input_data\n",
    "\n",
    "# Convert Question to Sequence\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "\n",
    "# Get the max length of Question after tokenization\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "\n",
    "# Do Post padding\n",
    "padded_questions = tensorflow.keras.utils.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "# Print shapes\n",
    "print( encoder_input_data.shape , maxlen_questions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13662, 42) 42\n"
     ]
    }
   ],
   "source": [
    "tokenized_answers = tokenizer.texts_to_sequences( answers_with_tags )\n",
    "\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    # Removing \"samaapt\" from the answer\n",
    "    tokenized_answers[i] = tokenized_answers[i][:-1] \n",
    "    \n",
    "# Find max len of answers    \n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "\n",
    "# Pad to the max length\n",
    "padded_answers = tensorflow.keras.utils.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "# Print shapes\n",
    "print( decoder_input_data.shape , maxlen_answers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13662, 42) 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers_with_tags )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    # removing \"prarambh\" from each answer\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:] \n",
    "\n",
    "# Pad to the max length\n",
    "padded_answers = tensorflow.keras.utils.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "\n",
    "decoder_output_data = np.array( padded_answers )\n",
    "# Print shapes\n",
    "print(decoder_output_data.shape , maxlen_answers )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/faq/single-faq/how-is-data-processed-by-an-lstm/\n",
    "# https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/ \n",
    "\n",
    "# n_input is maxlen_questions and n_output is maxlen_answers\n",
    "\n",
    "# Here is a question from MachineLearningMastery\n",
    "# 3 models were instantiated (train, infenc, infdec). \n",
    "# The define_models() function, you can see that model weights are shared between the models – but each model provides a different use case.\n",
    "\n",
    "def define_models(n_input, n_output, n_units):\n",
    "    # 1. define training encoder\n",
    "    encoder_inputs = tf.keras.layers.Input(shape=( n_input , )) \n",
    "    encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 64 , mask_zero=True ) (encoder_inputs) \n",
    "    encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( n_units , return_state=True )( encoder_embedding )\n",
    "    encoder_states = [ state_h , state_c ]\n",
    "\n",
    "    # 2. define training decoder\n",
    "\n",
    "    decoder_inputs = tf.keras.layers.Input(shape=( n_output ,  ))\n",
    "    decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 64 , mask_zero=True) (decoder_inputs) #Passing the input_length is optional here, but make use that the input_length is made constant by padding\n",
    "    decoder_lstm = tf.keras.layers.LSTM( n_units , return_state=True , return_sequences=True )\n",
    "    decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "    decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs )\n",
    "\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/faq/single-faq/how-is-data-processed-by-an-lstm/\n",
    "# https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/ \n",
    "\n",
    "# n_input is maxlen_questions and n_output is maxlen_answers\n",
    "\n",
    "# Here is a question from MachineLearningMastery\n",
    "# 3 models were instantiated (train, infenc, infdec). \n",
    "# The define_models() function, you can see that model weights are shared between the models – but each model provides a different use case.\n",
    "\n",
    "def define_models(n_input, n_output, n_units):\n",
    "    # 1. define training encoder\n",
    "    encoder_inputs = tf.keras.layers.Input(shape=( n_input , )) \n",
    "    encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 64 , mask_zero=True ) (encoder_inputs) \n",
    "    encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(n_units , return_state=True )( encoder_embedding )\n",
    "    encoder_states = [state_h,state_c]\n",
    "\n",
    "    # 2. define training decoder\n",
    "\n",
    "    decoder_inputs = tf.keras.layers.Input(shape=( n_output ,  ))\n",
    "    decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 64 , mask_zero=True) (decoder_inputs) #Passing the input_length is optional here, but make use that the input_length is made constant by padding\n",
    "    decoder_lstm = tf.keras.layers.LSTM( n_units , return_state=True , return_sequences=True )\n",
    "    decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "    decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "    decoder_outputs = decoder_dense ( decoder_outputs )\n",
    "\n",
    "    model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs )\n",
    "\n",
    "    # 3. Define Inference Encoder\n",
    "        # SHAPES : encoder_inputs- (1, 154) || encoder_states - (1, 100)\n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    # 4. Define Inference Decoder\n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( n_units ,)) \n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( n_units ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    decoder_inputs_inference = tf.keras.layers.Input(shape=(1,))\n",
    "    decoder_embedding_layer = tf.keras.layers.Embedding( VOCAB_SIZE, 64 , mask_zero=True )\n",
    "    decoder_inputs_embedding_inference = decoder_embedding_layer(decoder_inputs_inference)\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_embedding_inference , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # There will be 3 Inputs of SHAPES: decoder_inputs_inference-(1,1) || decoder_states_inputs- (1,100) , (1,100)\n",
    "    # There will be 3 outputs of SHAPES: decoder_outputs- (1,1,VOCAB_SIZE) || decoder_states - (1,100) , (1,100)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs_inference] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_43 (InputLayer)          [(None, 55)]         0           []                               \n",
      "                                                                                                  \n",
      " input_44 (InputLayer)          [(None, 42)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_20 (Embedding)       (None, 55, 64)       1926016     ['input_43[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_21 (Embedding)       (None, 42, 64)       1926016     ['input_44[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_18 (LSTM)                 [(None, 100),        66000       ['embedding_20[0][0]']           \n",
      "                                 (None, 100),                                                     \n",
      "                                 (None, 100)]                                                     \n",
      "                                                                                                  \n",
      " lstm_19 (LSTM)                 multiple             66000       ['embedding_21[0][0]',           \n",
      "                                                                  'lstm_18[0][1]',                \n",
      "                                                                  'lstm_18[0][2]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                multiple             3039494     ['lstm_19[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,023,526\n",
      "Trainable params: 7,023,526\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3 models were instantiated (train, infenc, infdec)\n",
    "\n",
    "train, infenc, infdec = define_models(maxlen_questions, maxlen_answers, 100)\n",
    "train.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='sparse_categorical_crossentropy')\n",
    "train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 138s 747ms/step - loss: 7.2009 - val_loss: 6.2068\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\n",
    "# Early stopping for avoiding overtraining of Neural Network\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "if not os.path.exists('../model/model_train.h5'):\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    history = train.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=64, epochs=1 , validation_split=0.20, callbacks=[es])\n",
    "    train.save('../model/model_train.h5') \n",
    "    infenc.save('../model/model_infenc.h5') \n",
    "    infdec.save('../model/model_infdec.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 55)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 42)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 55, 64)       1926016     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 42, 64)       1926016     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 100),        66000       ['embedding[0][0]']              \n",
      "                                 (None, 100),                                                     \n",
      "                                 (None, 100)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 42, 100),    66000       ['embedding_1[0][0]',            \n",
      "                                 (None, 100),                     'lstm[0][1]',                   \n",
      "                                 (None, 100)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 42, 30094)    3039494     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,023,526\n",
      "Trainable params: 7,023,526\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3 models were instantiated (train, infenc, infdec)\n",
    "\n",
    "train = define_models(maxlen_questions, maxlen_answers, 100)\n",
    "train.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='sparse_categorical_crossentropy')\n",
    "train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TFLite model for infenc model\n",
    "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(train)\n",
    "tf_lite_converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = tf_lite_converter.convert()\n",
    "\n",
    "# Write the model to the specified path\n",
    "TF_LITE_MODEL_FILE_NAME_1 = \"../model/tf_lite_infenc.tflite\"\n",
    "tflite_model_name_1 = TF_LITE_MODEL_FILE_NAME_1\n",
    "open(tflite_model_name_1, \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    tokens = tokenizer.texts_to_sequences([sentence])\n",
    "    return tensorflow.keras.utils.pad_sequences( tokens , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decode(encoded_seq):\n",
    "    li = [argmax(vector) for vector in encoded_seq]\n",
    "    # li will be a list like [12,2230,2345......(n_steps) times]\n",
    "    decoded_translation = ''\n",
    "    for value in li:\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if value == index:\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "\n",
    "    return decoded_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array,argmax\n",
    "def predict_answer(n_steps):\n",
    "    interpreter_1 = tf.lite.Interpreter(model_path = TF_LITE_MODEL_FILE_NAME_1)\n",
    "    input_details_1 = interpreter_1.get_input_details()\n",
    "    output_details_1 = interpreter_1.get_output_details()\n",
    "\n",
    "    interpreter_2 = tf.lite.Interpreter(model_path = TF_LITE_MODEL_FILE_NAME_2)\n",
    "    input_details_2 = interpreter_2.get_input_details()\n",
    "    output_details_2 = interpreter_2.get_output_details()\n",
    "\n",
    "    # Encoder Input\n",
    "    li = str_to_tokens( input( 'Enter question : ' ) )\n",
    "\n",
    "    # Changing the input to float32 bcoz the encoder input expects the type to be float32\n",
    "    li2= np.array(li, dtype=np.float32)\n",
    "\n",
    "    # Allocate tensor, set tensor, invoke, get tensor\n",
    "    interpreter_1.allocate_tensors()\n",
    "    interpreter_1.set_tensor(input_details_1[0]['index'], li2)\n",
    "\n",
    "    interpreter_1.invoke()\n",
    "\n",
    "    # Encoder Predictions\n",
    "    # SHAPES: tflite_model_predictions_1_0 - (1,100) || tflite_model_predictions_1_1 - (1,100)\n",
    "    tflite_model_predictions_1_0 = interpreter_1.get_tensor(output_details_1[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions_1_0.shape)\n",
    "\n",
    "    tflite_model_predictions_1_1 = interpreter_1.get_tensor(output_details_1[1]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions_1_1.shape)\n",
    "\n",
    "    output = list()\n",
    "    #n_steps = 10\n",
    "\n",
    "    # First input to the decoder\n",
    "    target_seq = np.zeros( ( 1 , 1 ), dtype=np.float32 ) \n",
    "    target_seq[0, 0] = tokenizer.word_index['prarambh']\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Allocate(only once), [set, invoke , get]\n",
    "        # First Input for the Decoder of shape (1, 100)\n",
    "        interpreter_2.allocate_tensors()\n",
    "        interpreter_2.set_tensor(input_details_2[0]['index'], tflite_model_predictions_1_0)\n",
    "\n",
    "        # Second Input for Decoder of shape (1, 100)\n",
    "        interpreter_2.set_tensor(input_details_2[1]['index'], tflite_model_predictions_1_1)\n",
    "\n",
    "        # Third input Decoder for Decoder of shape (1, 1)\n",
    "        interpreter_2.set_tensor(input_details_2[2]['index'], target_seq)\n",
    "\n",
    "        interpreter_2.invoke()\n",
    "\n",
    "        # Decoder Predictions\n",
    "        # SHAPES:  tflite_model_predictions_2_0- (1,1,VOCAB_SIZE) || tflite_model_predictions_2_1- (1,100) || tflite_model_predictions_2_2- (1,100) \n",
    "        tflite_model_predictions_2_0 = interpreter_2.get_tensor(output_details_2[0]['index'])\n",
    "        tflite_model_predictions_2_1 = interpreter_2.get_tensor(output_details_2[1]['index'])\n",
    "        tflite_model_predictions_2_2 = interpreter_2.get_tensor(output_details_2[2]['index'])\n",
    "\n",
    "        output.append(tflite_model_predictions_2_0[0, 0, :])\n",
    "        # Getting data ready for next iteration\n",
    "        index = np.argmax( tflite_model_predictions_2_0[0, 0, :] )\n",
    "        if index != tokenizer.word_index['samaapt']: # At a given iteration if the prediction is 'end', then the loop should break\n",
    "            target_seq = np.zeros( ( 1 , 1 ), dtype=np.float32 )  \n",
    "            target_seq[ 0 , 0 ] = index\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        tflite_model_predictions_1_0 = tflite_model_predictions_2_1\n",
    "        tflite_model_predictions_1_1 = tflite_model_predictions_2_2\n",
    "\n",
    "    answer_out = one_hot_decode(array(output))\n",
    "    print(\"Answer is : \", answer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.2.0",
   "language": "python",
   "name": "tf2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
